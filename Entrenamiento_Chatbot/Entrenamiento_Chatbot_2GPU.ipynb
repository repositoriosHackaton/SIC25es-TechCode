{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljIEkWgQuetq"
      },
      "outputs": [],
      "source": [
        "# Pensado para Kaggle noteboks\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        texto = self.texts[idx]\n",
        "        emocion = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            texto,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': encoding['input_ids'].squeeze(0)\n",
        "        }\n",
        "\n",
        "class EmotionalChatbotTrainer:\n",
        "    def __init__(self, model_name='gpt2-medium', learning_rate=5e-5):\n",
        "        # Cargar modelo y tokenizador\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "        # Configuración del dispositivo\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Si hay más de una GPU, utilizar DataParallel\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(f'Usando {torch.cuda.device_count()} GPUs')\n",
        "            self.model = torch.nn.DataParallel(self.model)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Configuración de hiperparámetros\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def preparar_dataloaders(self, train_file, test_file, batch_size=8, max_length=128):\n",
        "        # Cargar los CSVs\n",
        "        train_df = pd.read_csv(train_file)\n",
        "        test_df = pd.read_csv(test_file)\n",
        "\n",
        "        # Obtener los textos y las emociones\n",
        "        X_train = train_df['texto'].tolist()\n",
        "        y_train = train_df['emocion'].tolist()\n",
        "        X_test = test_df['texto'].tolist()\n",
        "        y_test = test_df['emocion'].tolist()\n",
        "\n",
        "        # Crear datasets\n",
        "        train_dataset = ChatbotDataset(X_train, y_train, self.tokenizer, max_length)\n",
        "        test_dataset = ChatbotDataset(X_test, y_test, self.tokenizer, max_length)\n",
        "\n",
        "        # Crear dataloaders\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        return train_dataloader, test_dataloader\n",
        "\n",
        "    def entrenar(self, train_dataloader, test_dataloader, epochs=3):\n",
        "        # Configurar el modelo para entrenamiento\n",
        "        self.model.train()\n",
        "\n",
        "        # Configurar el optimizador\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate, eps=1e-8)\n",
        "\n",
        "        # Configurar el scheduler de learning rate\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "        # Usar FP16 (16-bit precision) si es posible\n",
        "        scaler = torch.cuda.amp.GradScaler(init_scale=2.**15)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f'Época {epoch + 1}/{epochs}')\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch in tqdm(train_dataloader, desc='Entrenando'):\n",
        "                # Mover datos al dispositivo\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "                # Limpiar gradientes\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass con FP16\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "                    # Manejar la pérdida para DataParallel\n",
        "                    if isinstance(outputs, tuple):\n",
        "                        loss = outputs.loss.mean()\n",
        "                    else:\n",
        "                        loss = outputs[0].mean()\n",
        "\n",
        "                # Acumular pérdida\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Backward pass con FP16\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Clip de gradientes\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                # Actualizar parámetros\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                # Actualizar scheduler\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calcular pérdida promedio de entrenamiento\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "            print(f'Pérdida de entrenamiento: {avg_train_loss}')\n",
        "\n",
        "            # Evaluación\n",
        "            self._evaluar(test_dataloader)\n",
        "\n",
        "    def _evaluar(self, test_dataloader):\n",
        "        # Configurar el modelo para evaluación\n",
        "        self.model.eval()\n",
        "        total_eval_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dataloader, desc='Evaluando'):\n",
        "                # Mover datos al dispositivo\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "                # Evaluar con FP16\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "                    # Manejar la pérdida para DataParallel\n",
        "                    if isinstance(outputs, tuple):\n",
        "                        loss = outputs.loss.mean()\n",
        "                    else:\n",
        "                        loss = outputs[0].mean()\n",
        "\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "        # Calcular pérdida promedio de evaluación\n",
        "        avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
        "        print(f'Pérdida de evaluación: {avg_eval_loss}')\n",
        "\n",
        "    def guardar_modelo(self, ruta='chatbot_emocional_modelo'):\n",
        "        # Extraer el modelo base si está en DataParallel\n",
        "        modelo_guardar = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "\n",
        "        modelo_guardar.save_pretrained(ruta)\n",
        "        self.tokenizer.save_pretrained(ruta)\n",
        "        print(f'Modelo guardado en {ruta}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Definir los archivos CSV\n",
        "    # Solo se suben los archivos a kaggle y si no estoy mal no cambian las rutas\n",
        "    train_file = '/kaggle/input/chatbot/train_dataset.csv'\n",
        "    test_file = '/kaggle/input/chatbot/test_dataset.csv'\n",
        "\n",
        "    # Inicializar entrenador\n",
        "    trainer = EmotionalChatbotTrainer(model_name='gpt2-medium')\n",
        "\n",
        "    # Preparar dataloaders\n",
        "    train_dataloader, test_dataloader = trainer.preparar_dataloaders(train_file, test_file)\n",
        "\n",
        "    # Entrenar modelo\n",
        "    trainer.entrenar(train_dataloader, test_dataloader)\n",
        "\n",
        "    # Guardar modelo\n",
        "    trainer.guardar_modelo()"
      ]
    }
  ]
}